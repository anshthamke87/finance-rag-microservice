# ðŸš€ Finance RAG Microservice

**End-to-end Retrieval-Augmented Generation for Finance QA (FiQA)** â€” from raw data to a production-ready API.  
We: **downloaded & cleaned data â†’ built strong hybrid retrieval (BM25 + dense ANN + CE) â†’ fine-tuned the bi-encoder â†’ generated grounded answers with citations (FLAN-T5)** â†’ packaged everything into a **FastAPI** microservice.


---

## ðŸ”¥ Highlights

- **Clean, reproducible pipeline** split into 4 notebooks (NB1â€“NB4), easy to run in Colab.
- **Strong retriever**: BM25 + HNSW FAISS (inner product) + **cross-encoder reranking**.
- **Fine-tuning**: contrastive training on mined positive/negative pairs from FiQA train/dev.
- **RAG generation**: FLANâ€‘T5 with **cross-encoderâ€“picked passages** (multiple per doc) and a **token budget** for robust grounding.
- **Citations**: answers include `[DOC:ID]` tags tied to passages so reviewers can verify.
- **Service**: small **FastAPI** app with a single `/answer` endpoint.
- **Artifacts + reports** saved under `artifacts/`, `indices/`, `runs/`, and `reports/`.

---

## ðŸ“Š Results (Test)

**Retrieval (hybrid + CE, fineâ€‘tuned)**  
- nDCG@10: **0.3794**  
- MRR@10: **0.4526**  
- Recall@100: **0.6247**

**RAG (CEâ€‘picked passages, tuned prompt)**  
- n_predictions: **648**  
- Support@12: **0.6852**  
- EvidenceRecall@12: **0.4771**  
- ContextContainment: **0.9139**

> _Support@k:_ at least one of the topâ€‘k retrieved docs is truly relevant.  
> _EvidenceRecall@k:_ share of all relevant docs that appear in topâ€‘k.  
> _ContextContainment:_ % of answer tokens found in the provided passages.

---

## ðŸ§  What we built (endâ€‘toâ€‘end)

### NB1 â€” Data & Baseline
- ðŸ“¥ Downloaded **FiQA** (BeIR) corpus + queries + qrels; saved **train/dev/test** splits to Drive.
- ðŸ§© Created **passages** via chunking; saved as `artifacts/passages.parquet`.
- ðŸ”Ž Baseline **BM25** (Rankâ€‘BM25) over passages; reported nDCG/MRR/Recall; stored report JSON.

### NB2 â€” Hybrid Retrieval + Tuning + Fineâ€‘tune
- ðŸ§® Built **dense ANN** with **FAISS HNSW (inner product)** on MiniLM embeddings.
- ðŸ”€ **Fusion** of BM25 + ANN via minâ€‘max normalization (Î± mixing).
- ðŸ¤ **Crossâ€‘encoder reranking** (`cross-encoder/ms-marco-MiniLM-L-6-v2`) on topâ€‘N candidates.
- âš™ï¸ **Hyperparam grid** (Î±, efSearch, CE budget) â†’ dev leaderboard â†’ pick best.
- ðŸŽ¯ **Fineâ€‘tuned** the biâ€‘encoder using mined positive/negative pairs (CEâ€‘guided), with inâ€‘notebook checkpoints.
- ðŸ§¾ Saved: FAISS index, embeddings, tuned config (`retrieval_final_selection.json`), and final IR metrics.

### NB3 â€” RAG Generation + Eval
- ðŸ“š **Context selection**: For each top doc, **crossâ€‘encoder scores multiple passages**, keeps the best 2â€“3 per doc, then globally picks top passages under a **token budget** (e.g., 1,400 tokens). This maximizes the chance the **answerâ€‘bearing** sentence is present.
- ðŸ§¾ **Prompting** (FLANâ€‘T5): concise 1â€“3 sentence answers; optional **generalâ€‘knowledge fallback** labeled with â€œBased on general knowledge,â€ to reduce abstains while keeping transparency.
- âœï¸ **Decoding**: beam search, `min_new_tokens` to avoid 1â€‘word replies, light `length_penalty`.
- âœ… **Evaluation**: Support@k, EvidenceRecall@k, ContextContainment, plus quick abstain/length stats.
- ðŸ“¦ Saved predictions (`runs/*.jsonl`), evals (`reports/*.json`), and a **run manifest** + **bestâ€‘run pointer**.

### NB4 â€” Microservice Packaging (this notebook)
- ðŸ“¦ Created **FastAPI** app: loads retriever + generator and exposes `POST /answer`.
- ðŸ”§ Tiny **run scripts** and a **HOWTO** for local use.
- ðŸ“ Autoâ€‘generated this **README.md** from your saved metrics.
- (Optional) Smoke tests in Colab using `fastapi.testclient`.

---

## ðŸ§© Design Choices (and why)

- **BM25 + Dense + CE**: BM25 excels at lexicon overlap; dense covers synonyms/semantics; CE provides precise pairwise scoring on top of candidates. Together, these deliver high recall **and** precision.
- **HNSW + inner product**: scalable ANN for queryâ€‘time speed, with normalized vectors for cosineâ€‘like similarity.
- **CEâ€‘picked passages (multiple per doc)**: Avoids the â€œbest single passage per docâ€ trap; we pick **several** likely answerâ€‘bearing snippets then pack to a budget â†’ lower abstain rate, better grounding.
- **Prompt with labeled fallback**: When context is thin, allowing a **clearly marked** generalâ€‘knowledge note improves utility while preserving transparency.
- **Deterministic decoding**: `num_beams=4`, `do_sample=False`, `min_new_tokens` keeps answers consistent and nonâ€‘trivial.

---

## ðŸ§ª Reproducibility

- Random seeds set for NumPy / Torch where feasible.
- All major artifacts and configs saved under the repo (`artifacts/`, `indices/`, `reports/`, `runs/`).
- Final tuned retrieval config lives in: `reports/retrieval_final_selection.json`  
- Best RAG run pointer in: `reports/rag_best_run.json`

> If GitHub size is an issue, use Git LFS for `indices/` and `artifacts/`, or regenerate via notebooks.

---

## ðŸ“ Repo Layout

```
.
â”œâ”€â”€ src/service/               # FastAPI app + retriever/generator wrappers
â”œâ”€â”€ scripts/                   # serve + curl examples
â”œâ”€â”€ notebooks/                 # HOWTO + (run NB1..NB3 in Colab)
â”œâ”€â”€ reports/                   # JSON metrics (IR + RAG) + manifests
â”œâ”€â”€ runs/                      # Predictions (JSONL)
â”œâ”€â”€ artifacts/                 # passages.parquet, embeddings, fine-tuned model
â”œâ”€â”€ indices/faiss_hnsw/        # FAISS HNSW index + mapping
â””â”€â”€ eval/qrels/                # FiQA qrels & queries (train/dev/test)
```

---

## â–¶ï¸ Run the API locally

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
./scripts/serve.sh
# new terminal:
./scripts/curl_example.sh
```

> The service loads models at runtime. âœ… GPU recommended; CPU works but slower.

---

## ðŸ”— API

`POST /answer`

**Request**:
```json
{
  "question": "How do I transfer a 401k after closing a business?",
  "top_k": 12,
  "allow_external_knowledge": true
}
```

**Response**:
```json
{
  "question": "...",
  "answer": "...",
  "citations": [{"doc_id":"...", "passage_idx":123}, ...],
  "used_k": 12
}
```

---

## ðŸ“š Credits & Licenses

- Dataset: **FiQA** (BeIR); academic use â€” please respect original licenses.
- Models: `sentence-transformers/all-MiniLM-L6-v2`, `cross-encoder/ms-marco-MiniLM-L-6-v2`, `google/flan-t5-large`.
- Libraries: FAISS, FastAPI, Transformers, Sentence-Transformers, Rank-BM25.

---

_Questions or feedback? Happy to iterate!_ âœ¨
