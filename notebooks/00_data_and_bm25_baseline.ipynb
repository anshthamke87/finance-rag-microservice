{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpBpYjAel0cQlCwWIse9UJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Mount Drive so later notebooks pick up saved artifacts\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from pathlib import Path\n","\n","PROJECT_NAME = \"finance-rag-microservice\"\n","ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","\n","# Create the exact structure from the assignment\n","paths = [\n","    ROOT / \"app\",                      # FastAPI or Streamlit demo\n","    ROOT / \"cfg\",                      # dataset.yaml, retrieval.yaml, generation.yaml\n","    ROOT / \"data\",                     # raw dataset downloads here\n","    ROOT / \"indices\" / \"bm25\",\n","    ROOT / \"indices\" / \"faiss_hnsw\",\n","    ROOT / \"artifacts\",                # passages.parquet, embeddings later\n","    ROOT / \"runs\",                     # bm25.trec, hybrid_ce.trec\n","    ROOT / \"eval\" / \"qrels\",           # qrels/test etc.\n","    ROOT / \"notebooks\",                # 01_build_index.ipynb etc. (you'll save the Colab as this)\n","    ROOT / \"reports\",                  # ir_*.json, ragas.json, latency.csv...\n","    ROOT / \"docs\",                     # prompt_library.md, system_overview.png\n","    ROOT / \"tests\",                    # test_retrieval.py, test_grounding.py\n","]\n","\n","for p in paths:\n","    p.mkdir(parents=True, exist_ok=True)\n","\n","# Create small placeholder files the assignment expects\n","# (These are minimal stubs; you can expand later.)\n","(ROOT / \"README.md\").write_text(\"# Finance RAG Microservice\\n\\nSee notebooks for setup.\\n\")\n","(ROOT / \"metrics.md\").write_text(\"# Metrics\\n\\n(Consolidated tables will be added here.)\\n\")\n","(ROOT / \"Makefile\").write_text(\n","    \"install:\\n\\tpip install -r requirements.txt\\n\\n\"\n","    \"run-api:\\n\\tuvicorn app.main:app --reload --port 8000\\n\"\n",")\n","(ROOT / \"Dockerfile\").write_text(\n","    \"FROM python:3.11-slim\\nWORKDIR /app\\nCOPY requirements.txt .\\n\"\n","    \"RUN pip install --no-cache-dir -r requirements.txt\\nCOPY . .\\n\"\n","    \"CMD [\\\"uvicorn\\\", \\\"app.main:app\\\", \\\"--host\\\", \\\"0.0.0.0\\\", \\\"--port\\\", \\\"8000\\\"]\\n\"\n",")\n","(ROOT / \"requirements.txt\").write_text(\n","    \"\\n\".join([\n","        # minimal for this notebook\n","        \"numpy>=1.26,<2\",\n","        \"pandas>=2.2\",\n","        \"tqdm>=4.66\",\n","        \"pyarrow>=15\",\n","        \"rank-bm25>=0.2.2\",\n","        # future notebooks (kept here for reproducibility)\n","        \"faiss-cpu>=1.8.0\",                 # FAISS HNSW (NB02)\n","        \"sentence-transformers>=3.0.1\",     # MiniLM embeddings (NB02)\n","        \"transformers>=4.42.0\",             # Flan-T5 (NB03)\n","        \"accelerate>=0.33.0\",\n","        \"ragas>=0.1.14\",                    # RAG metrics (NB03)\n","        \"fastapi>=0.112.0\", \"uvicorn>=0.30.0\",  # API demo (NB05)\n","    ])\n",")\n","# Baseline cfg (edit later if needed)\n","(ROOT / \"cfg\" / \"dataset.yaml\").write_text(\n","    \"name: beir-fiqa-2018\\nuse_full_corpus: true\\nchunk_tokens: 512\\nchunk_stride: 128\\nseed: 42\\nrag_eval_n: 600\\n\"\n",")\n","(ROOT / \"cfg\" / \"retrieval.yaml\").write_text(\n","    \"bm25:\\n  k1: 1.2\\n  b: 0.75\\n  top_k_candidates: 1000\\nhybrid:\\n  bm25_top: 500\\n  ann_top: 100\\n\"\n","    \"rerank:\\n  top_for_ce: 50\\n  model: cross-encoder/ms-marco-MiniLM-L-6-v2\\n\"\n",")\n","(ROOT / \"cfg\" / \"generation.yaml\").write_text(\n","    \"model: google/flan-t5-base\\nk_contexts: 4\\nmax_new_tokens: 256\\ntemperature: 0.1\\nabstain_threshold: 0.2\\n\"\n",")\n","\n","print(\"✅ Repo scaffold created at:\", ROOT)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9o-vyJBXE5Oe","executionInfo":{"status":"ok","timestamp":1756060044551,"user_tz":240,"elapsed":738,"user":{"displayName":"Ansh Thamke","userId":"00311201502062895987"}},"outputId":"e6682871-acd5-48f0-9481-f724c6c84225"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Repo scaffold created at: /content/drive/MyDrive/finance-rag-microservice\n"]}]},{"cell_type":"code","source":["!pip -q install datasets rank-bm25 pandas tqdm pyarrow"],"metadata":{"id":"rwOu0iMFE_5q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Robust .zip download (reuses if present) — BEIR hosts fiqa.zip (not .tar.gz)\n","import urllib.request, os, zipfile, shutil\n","from pathlib import Path\n","\n","DATA_DIR = ROOT / \"data\"\n","ZIP_PATH = DATA_DIR / \"fiqa.zip\"\n","EXTRACT_DIR = DATA_DIR / \"fiqa\"\n","URL = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fiqa.zip\"\n","\n","def download_if_needed(url: str, dest: Path, min_bytes: int = 1_000_000):\n","    dest.parent.mkdir(parents=True, exist_ok=True)\n","    if dest.exists() and dest.stat().st_size >= min_bytes:\n","        print(f\"Found existing zip: {dest} ({dest.stat().st_size/1_048_576:.1f} MB)\")\n","        return\n","    print(f\"Downloading:\\n  {url}\\n  → {dest}\")\n","    urllib.request.urlretrieve(url, dest)\n","    if dest.stat().st_size < min_bytes:\n","        raise RuntimeError(f\"Download seems incomplete (size={dest.stat().st_size} bytes).\")\n","\n","def safe_extract_zip(src: Path, out_dir: Path):\n","    # Clean any previous extraction to avoid mixed states\n","    if EXTRACT_DIR.exists():\n","        shutil.rmtree(EXTRACT_DIR)\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","\n","    with zipfile.ZipFile(src) as z:\n","        # Path traversal guard\n","        for m in z.infolist():\n","            extracted_path = (out_dir / m.filename).resolve()\n","            if not str(extracted_path).startswith(str(out_dir.resolve())):\n","                raise Exception(\"Blocked path traversal during extract.\")\n","        z.extractall(out_dir)\n","\n","# 1) Download (or reuse)\n","download_if_needed(URL, ZIP_PATH)\n","\n","# 2) Extract into data/fiqa\n","print(f\"Extracting {ZIP_PATH.name} → {DATA_DIR} ...\")\n","safe_extract_zip(ZIP_PATH, DATA_DIR)\n","\n","# Some archives already contain 'fiqa/' as the top-level; ensure final path is data/fiqa\n","if not EXTRACT_DIR.exists():\n","    # Find a likely inner dir and rename\n","    candidates = [p for p in DATA_DIR.iterdir() if p.is_dir() and p.name.lower().startswith(\"fiqa\")]\n","    if candidates:\n","        candidates[0].rename(EXTRACT_DIR)\n","\n","# 3) Sanity check expected files\n","expected = [\n","    EXTRACT_DIR / \"corpus.jsonl\",\n","    EXTRACT_DIR / \"queries.jsonl\",\n","    EXTRACT_DIR / \"qrels\" / \"test.tsv\",\n","]\n","for p in expected:\n","    if not p.exists():\n","        raise FileNotFoundError(f\"Missing expected file: {p}\")\n","print(\"✅ FiQA files present:\", [str(p) for p in expected])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAQeX_5AFDOm","executionInfo":{"status":"ok","timestamp":1756060054832,"user_tz":240,"elapsed":1017,"user":{"displayName":"Ansh Thamke","userId":"00311201502062895987"}},"outputId":"ed275e1a-ac3e-4ee2-c5aa-30d9a6285ab4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing zip: /content/drive/MyDrive/finance-rag-microservice/data/fiqa.zip (17.1 MB)\n","Extracting fiqa.zip → /content/drive/MyDrive/finance-rag-microservice/data ...\n","✅ FiQA files present: ['/content/drive/MyDrive/finance-rag-microservice/data/fiqa/corpus.jsonl', '/content/drive/MyDrive/finance-rag-microservice/data/fiqa/queries.jsonl', '/content/drive/MyDrive/finance-rag-microservice/data/fiqa/qrels/test.tsv']\n"]}]},{"cell_type":"code","source":["# D) Load FiQA once, save split artifacts (train/dev/test) for later use\n","#    This does NOT run BM25; it just prepares data.\n","\n","import json, csv\n","from pathlib import Path\n","\n","# EXTRACT_DIR and ROOT come from earlier cells:\n","# EXTRACT_DIR = ROOT / \"data\" / \"fiqa\"\n","# ROOT = /content/drive/MyDrive/finance-rag-microservice\n","\n","# 1) Load corpus.jsonl (full doc set)\n","corpus = {}\n","with open(EXTRACT_DIR / \"corpus.jsonl\", \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        rec = json.loads(line)\n","        corpus[str(rec[\"_id\"])] = {\n","            \"title\": rec.get(\"title\", \"\") or \"\",\n","            \"text\": rec.get(\"text\", \"\") or \"\",\n","            \"metadata\": rec.get(\"metadata\", {}) or {},\n","        }\n","\n","# 2) Load all queries (we'll filter by split)\n","all_queries = {}\n","with open(EXTRACT_DIR / \"queries.jsonl\", \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        rec = json.loads(line)\n","        all_queries[str(rec[\"_id\"])] = rec[\"text\"]\n","\n","# 3) Helper to read qrels TSV (handles header and 3- or 4-column format)\n","def _is_header(row):\n","    return bool(row) and row[0].strip().lower().startswith((\"query\",\"qid\"))\n","\n","def _load_qrels_file(tsv_path: Path):\n","    out = {}\n","    with open(tsv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n","        reader = csv.reader(f, delimiter=\"\\t\")\n","        for row in reader:\n","            if not row or all(not x.strip() for x in row) or _is_header(row):\n","                continue\n","            # Accept 3-col (qid, docid, score) OR 4+ col (qid, Q0, docid, score)\n","            if len(row) == 3:\n","                qid, did, score = row\n","            elif len(row) >= 4:\n","                qid, did, score = row[0], row[2], row[-1]\n","            else:\n","                continue\n","            try:\n","                rel = int(float(score))\n","            except ValueError:\n","                continue\n","            out.setdefault(qid, {})[did] = rel\n","    return out\n","\n","qrels_dir = EXTRACT_DIR / \"qrels\"\n","qrels_train = _load_qrels_file(qrels_dir / \"train.tsv\")\n","qrels_dev   = _load_qrels_file(qrels_dir / \"dev.tsv\")\n","qrels_test  = _load_qrels_file(qrels_dir / \"test.tsv\")\n","\n","# 4) Split-filtered queries (keep only queries that appear in that split’s qrels)\n","queries_train = {qid: all_queries[qid] for qid in qrels_train if qid in all_queries}\n","queries_dev   = {qid: all_queries[qid] for qid in qrels_dev   if qid in all_queries}\n","queries_test  = {qid: all_queries[qid] for qid in qrels_test  if qid in all_queries}\n","\n","def _count_pairs(qdict): return sum(len(v) for v in qdict.values())\n","\n","print(\"Docs:\", len(corpus))\n","print(\"Train queries:\", len(queries_train), \"qrels:\", _count_pairs(qrels_train))\n","print(\"Dev   queries:\", len(queries_dev),   \"qrels:\", _count_pairs(qrels_dev))\n","print(\"Test  queries:\", len(queries_test),  \"qrels:\", _count_pairs(qrels_test))\n","\n","# 5) Persist split JSONs to eval/qrels/ (the scaffold’s evaluation folder)\n","outdir = ROOT / \"eval\" / \"qrels\"\n","outdir.mkdir(parents=True, exist_ok=True)\n","\n","(outdir / \"fiqa_queries_train.json\").write_text(json.dumps(queries_train))\n","(outdir / \"fiqa_queries_dev.json\").write_text(json.dumps(queries_dev))\n","(outdir / \"fiqa_queries_test.json\").write_text(json.dumps(queries_test))\n","\n","(outdir / \"fiqa_qrels_train.json\").write_text(json.dumps(qrels_train))\n","(outdir / \"fiqa_qrels_dev.json\").write_text(json.dumps(qrels_dev))\n","(outdir / \"fiqa_qrels_test.json\").write_text(json.dumps(qrels_test))\n","\n","# Optional: save all-queries mapping for convenience later\n","(outdir / \"fiqa_queries_all.json\").write_text(json.dumps(all_queries))\n","\n","# 6) Save a small manifest for quick reference in README/reports\n","manifest = {\n","    \"docs\": len(corpus),\n","    \"splits\": {\n","        \"train\": {\"queries\": len(queries_train), \"qrels_pairs\": _count_pairs(qrels_train)},\n","        \"dev\":   {\"queries\": len(queries_dev),   \"qrels_pairs\": _count_pairs(qrels_dev)},\n","        \"test\":  {\"queries\": len(queries_test),  \"qrels_pairs\": _count_pairs(qrels_test)},\n","    }\n","}\n","(ROOT / \"reports\" / \"fiqa_splits_manifest.json\").write_text(json.dumps(manifest, indent=2))\n","print(\"Saved split JSONs →\", outdir)\n","print(\"Saved manifest   →\", ROOT / \"reports\" / \"fiqa_splits_manifest.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UllgLNzIFFVj","executionInfo":{"status":"ok","timestamp":1756063512760,"user_tz":240,"elapsed":695,"user":{"displayName":"Ansh Thamke","userId":"00311201502062895987"}},"outputId":"6176f99d-f031-4ae9-f9b3-ad6d09347156"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Docs: 57638\n","Train queries: 5500 qrels: 14166\n","Dev   queries: 500 qrels: 1238\n","Test  queries: 648 qrels: 1706\n","Saved split JSONs → /content/drive/MyDrive/finance-rag-microservice/eval/qrels\n","Saved manifest   → /content/drive/MyDrive/finance-rag-microservice/reports/fiqa_splits_manifest.json\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","import pandas as pd\n","\n","CHUNK_TOKENS = 512\n","CHUNK_STRIDE = 128\n","\n","def tokenize_simple(s: str):  # simple & fast\n","    return s.split()\n","\n","def chunk_tokens(tokens, win=CHUNK_TOKENS, stride=CHUNK_STRIDE):\n","    i, n = 0, len(tokens)\n","    while i < n:\n","        j = min(i + win, n)\n","        yield tokens[i:j]\n","        if j >= n:\n","            break\n","        i += (win - stride)\n","\n","rows = []\n","for doc_id, d in tqdm(corpus.items(), desc=\"Chunking docs\"):\n","    text = ((d.get(\"title\") or \"\") + \"\\n\" + (d.get(\"text\") or \"\")).strip()\n","    toks = tokenize_simple(text)\n","    if not toks:\n","        continue\n","    for window in chunk_tokens(toks):\n","        rows.append({\n","            \"doc_id\": doc_id,\n","            \"passage\": \" \".join(window)\n","        })\n","\n","passages_df = pd.DataFrame(rows)\n","passages_parquet = ROOT / \"artifacts\" / \"passages.parquet\"\n","passages_df.to_parquet(passages_parquet, index=False)\n","print(\"Saved passages:\", passages_parquet, \"(\", len(passages_df), \"rows )\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipmV9nVwFG-l","executionInfo":{"status":"ok","timestamp":1756063704272,"user_tz":240,"elapsed":2786,"user":{"displayName":"Ansh Thamke","userId":"00311201502062895987"}},"outputId":"9eb747b6-e889-4538-b9ce-29ba984b8dfb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Chunking docs: 100%|██████████| 57638/57638 [00:01<00:00, 40255.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saved passages: /content/drive/MyDrive/finance-rag-microservice/artifacts/passages.parquet ( 59018 rows )\n"]}]},{"cell_type":"code","source":["# BM25-3) Build BM25 index\n","from rank_bm25 import BM25Okapi\n","\n","passage_tokens = [p.split() for p in passages_df[\"passage\"].tolist()]\n","passage_doc_ids = passages_df[\"doc_id\"].tolist()\n","\n","bm25 = BM25Okapi(passage_tokens)\n","print(\"BM25 index over passages ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pwbxlOJjjniY","executionInfo":{"status":"ok","timestamp":1756063775860,"user_tz":240,"elapsed":4735,"user":{"displayName":"Ansh Thamke","userId":"00311201502062895987"}},"outputId":"54eb5c7f-69b3-497b-b6fb-7837bce0d389"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["BM25 index over passages ready.\n"]}]},{"cell_type":"code","source":["# BM25-4) Retrieval (passage scores → max-pool per doc) and write TREC run\n","import numpy as np\n","from tqdm import tqdm\n","\n","def bm25_doc_run(query_text, top_passages=2000, top_docs=1000):\n","    q_toks = query_text.split()\n","    scores = bm25.get_scores(q_toks)  # per passage\n","    k = min(top_passages, len(scores))\n","    top_idx = np.argpartition(scores, -k)[-k:]\n","    doc2score = {}\n","    for idx in top_idx:\n","        did = passage_doc_ids[idx]\n","        s = float(scores[idx])\n","        if (did not in doc2score) or (s > doc2score[did]):\n","            doc2score[did] = s\n","    return sorted(doc2score.items(), key=lambda x: x[1], reverse=True)[:top_docs]\n","\n","run_path = ROOT / \"runs\" / \"bm25_test.trec\"\n","with open(run_path, \"w\", encoding=\"utf-8\") as out:\n","    for qid, qtext in tqdm(queries.items(), desc=\"Retrieving (BM25→doc) [test]\"):\n","        ranked = bm25_doc_run(qtext, top_passages=2000, top_docs=1000)\n","        for rank, (doc_id, score) in enumerate(ranked, start=1):\n","            out.write(f\"{qid} Q0 {doc_id} {rank} {score:.6f} bm25_passagemax\\n\")\n","print(\"Wrote run:\", run_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXl2NvQmjqmv","executionInfo":{"status":"ok","timestamp":1756063991552,"user_tz":240,"elapsed":210207,"user":{"displayName":"Ansh Thamke","userId":"00311201502062895987"}},"outputId":"22fb6c3e-4bdd-4e54-b930-060537d77111"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Retrieving (BM25→doc) [test]: 100%|██████████| 648/648 [03:30<00:00,  3.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Wrote run: /content/drive/MyDrive/finance-rag-microservice/runs/bm25_test.trec\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# BM25-5) Metrics + report\n","from math import log2\n","import json\n","\n","# Load run as qid -> ranked list\n","qid_to_ranked = {}\n","with open(run_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        qid, _, docid, rank, score, _ = line.strip().split()\n","        qid_to_ranked.setdefault(qid, []).append(docid)\n","\n","def ndcg_at_k(qrels_for_q, ranked, k=10):\n","    rels = [1 if (doc in qrels_for_q and qrels_for_q[doc] > 0) else 0 for doc in ranked[:k]]\n","    dcg = sum(rel / log2(i+2) for i, rel in enumerate(rels))\n","    ideal = [1] * min(k, sum(1 for r in qrels_for_q.values() if r > 0))\n","    idcg = sum(rel / log2(i+2) for i, rel in enumerate(ideal))\n","    return (dcg / idcg) if idcg > 0 else 0.0\n","\n","def mrr_at_k(qrels_for_q, ranked, k=10):\n","    for i, doc in enumerate(ranked[:k], start=1):\n","        if doc in qrels_for_q and qrels_for_q[doc] > 0:\n","            return 1.0 / i\n","    return 0.0\n","\n","def recall_at_k(qrels_for_q, ranked, k=100):\n","    rel_docs = {d for d, r in qrels_for_q.items() if r > 0}\n","    if not rel_docs:\n","        return None\n","    found = sum(1 for d in ranked[:k] if d in rel_docs)\n","    return found / len(rel_docs)\n","\n","n, ndcg_sum, mrr_sum, recall_sum, recall_cnt = 0, 0.0, 0.0, 0.0, 0\n","for qid, rels in qrels.items():\n","    ranked = qid_to_ranked.get(qid, [])\n","    if not ranked:\n","        continue\n","    n += 1\n","    ndcg_sum += ndcg_at_k(rels, ranked, k=10)\n","    mrr_sum  += mrr_at_k(rels, ranked, k=10)\n","    r = recall_at_k(rels, ranked, k=100)\n","    if r is not None:\n","        recall_sum += r\n","        recall_cnt += 1\n","\n","metrics = {\n","    \"n_evaluated_queries\": n,\n","    \"nDCG@10\": round(ndcg_sum / n if n else 0.0, 4),\n","    \"MRR@10\":  round(mrr_sum / n if n else 0.0, 4),\n","    \"Recall@100\": round(recall_sum / recall_cnt if recall_cnt else 0.0, 4),\n","}\n","print(metrics)\n","\n","report = {\n","    \"dataset\": \"BEIR FiQA (test split)\",\n","    \"chunking\": {\"tokens\": 512, \"stride\": 128},\n","    \"bm25\": \"rank-bm25 over passages; doc score = max passage score\",\n","    \"run_file\": str(run_path),\n","    \"counts\": {\n","        \"passages\": int(len(passages_df)),\n","        \"test_queries\": int(len(queries)),\n","        \"test_qrels_pairs\": int(sum(len(v) for v in qrels.values())),\n","    },\n","    \"metrics\": metrics,\n","}\n","rep_path = ROOT / \"reports\" / \"ir_bm25_test.json\"\n","rep_path.write_text(json.dumps(report, indent=2))\n","print(\"Saved:\", rep_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BN7YhPpZjtGY","executionInfo":{"status":"ok","timestamp":1756063992644,"user_tz":240,"elapsed":1080,"user":{"displayName":"Ansh Thamke","userId":"00311201502062895987"}},"outputId":"18537952-6ca7-4151-950b-6e6e36bad025"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["{'n_evaluated_queries': 648, 'nDCG@10': 0.1365, 'MRR@10': 0.1697, 'Recall@100': 0.3203}\n","Saved: /content/drive/MyDrive/finance-rag-microservice/reports/ir_bm25_test.json\n"]}]},{"cell_type":"code","source":["print(\"✅ BM25 baseline (test) complete.\")\n","print(\"Artifacts:\")\n","print(\" - passages:\", str(ROOT / \"artifacts\" / \"passages.parquet\"))\n","print(\" - run file:\", str(ROOT / \"runs\" / \"bm25_test.trec\"))\n","print(\" - report:  \", str(ROOT / \"reports\" / \"ir_bm25_test.json\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1096eShjvxM","executionInfo":{"status":"ok","timestamp":1756063992681,"user_tz":240,"elapsed":19,"user":{"displayName":"Ansh Thamke","userId":"00311201502062895987"}},"outputId":"a5b51303-fa04-4be3-ba1f-28af18b1e5fa"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ BM25 baseline (test) complete.\n","Artifacts:\n"," - passages: /content/drive/MyDrive/finance-rag-microservice/artifacts/passages.parquet\n"," - run file: /content/drive/MyDrive/finance-rag-microservice/runs/bm25_test.trec\n"," - report:   /content/drive/MyDrive/finance-rag-microservice/reports/ir_bm25_test.json\n"]}]}]}